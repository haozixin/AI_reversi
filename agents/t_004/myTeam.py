import operator

from Reversi.reversi_model import ReversiGameRule
from Reversi.reversi_utils import Cell, GRID_SIZE, boardToString
from template import Agent, GameRule
import random


class myAgent(Agent):
    def __init__(self, _id):
        super().__init__(_id)
        self.rule = ReversiGameRule(2)
        # make sure the current agent id is correct


        self.weights_table = self.generate_static_weights(1)


    def SelectAction(self, actions, game_state):
        # we use BLACK as the first player and WHITE as the second player
        # we use the following heuristic to evaluate the board
        # 1. the number of pieces on the board
        # 2. the number of pieces on the corner
        # 3. the number of pieces on the edge

        self.rule.current_game_state = game_state
        self.rule.current_agent_index = self.id
        self.rule.agent_colors = game_state.agent_colors
        # remove redundant actions
        actions = list(set(actions))

        print(boardToString(game_state.board, 8))



        # find the best action
        best_action = None
        best_h_value = float("-inf")
        for action in actions:
            if action == "Pass" and len(actions) == 1:
                return action
            else:
                # get the new state after performing the action (the new state is in Min layer)
                new_state = self.rule.generateSuccessor(game_state, action, self.rule.getCurrentAgentIndex())
                print(boardToString(new_state.board, 8))
                h_value = self.minimax(new_state, 3, -float("inf"), float("inf"), False)
                if h_value > best_h_value:
                    best_h_value = h_value
                    best_action = action
        return best_action

        # return random.choice(actions)


    #  alpha beta pruning
    def minimax(self, gameState, depth, alpha, beta, maximizingPlayer):
        """
        This is a minimax algorithm with alpha beta pruning.
        """
        # if the game is over or the depth is 0, return the score
        if depth == 0 or self.rule.gameEnds():
            # TODO: add heuristic
            # return self.rule.calScore(gameState, self.id)
            return self.heuristic_function(gameState)

        # if it is the maximizing player
        if maximizingPlayer:
            max_value = -float("inf")
            # get all legal actions from the state and choose the best one for itself.
            actions = self.rule.getLegalActions(gameState, self.rule.getCurrentAgentIndex())
            # for each child of position/gameState
            states = self.next_layer(gameState, actions, self.rule.getCurrentAgentIndex())
            for child_state in states:
                # get the value of the action
                value = self.minimax(child_state, depth - 1, alpha, beta, False)
                max_value = max(value, max_value)
                alpha = max(alpha, value)
                if alpha >= beta:
                    break
            return max_value

        # if it is the minimizing player
        else:
            min_value = float("inf")
            # get all legal actions of the opponent and choose the best one for itself.
            actions = self.rule.getLegalActions(gameState, self.rule.getNextAgentIndex())
            # for each child of position/gameState
            states = self.next_layer(gameState, actions, self.rule.getNextAgentIndex())
            for child_state in states:
                value = self.minimax(child_state, depth - 1, alpha, beta, True)
                min_value = min(min_value, value)
                beta = min(beta, value)
                if alpha >= beta:
                    break
            return min_value


    def next_layer(self, game_state, actions, agent_id):
        """
        Helper function for minmax algorithm.
        Return new states(children of current position) that is generated by performing the actions on the Current game state
        """
        next_states = []
        for action in actions:
            # child of position
            new_state = self.rule.generateSuccessor(game_state, action, agent_id)
            next_states.append(new_state)

        # return the child of position
        return next_states


    def heuristic_function(self, game_state):
        return self.corner_captured(game_state)


    def coin_parity(self, game_state):
        """
        Component of the heuristic function.
        This is a heuristic function that returns the difference between the number of pieces of the current player and the
        number of pieces of the opponent.

        Be suitable for the end game.(more weight when the game is close to the end)
        """

        current_player_pieces = self.rule.calScore(game_state, self.rule.getCurrentAgentIndex())
        # get the number of pieces of the opponent
        opponent_pieces = self.rule.calScore(game_state, self.rule.getNextAgentIndex())
        # return the difference
        result = (current_player_pieces - opponent_pieces) * 100 / (current_player_pieces + opponent_pieces)

        return result

    def utility_value(self, game_state):
        """
        Component of the heuristic function.
        """
        # get the current player's color
        current_player_color = self.rule.agent_colors[self.rule.getCurrentAgentIndex()]
        # get the opponent's color
        opponent_color = self.rule.agent_colors[self.rule.getNextAgentIndex()]
        # get the number of pieces of the current player
        board = game_state.board
        max_player_score = 0
        min_player_score = 0
        for i in range(GRID_SIZE):
            for j in range(GRID_SIZE):
                if board[i][j] == current_player_color:
                    max_player_score += self.weights_table[i][j]
                elif board[i][j] == opponent_color:
                    min_player_score -= self.weights_table[i][j]
        return max_player_score - min_player_score


    def corner_captured(self, game_state):
        game_state = game_state.board
        # get the current player's color
        current_player_color = self.rule.agent_colors[self.rule.getCurrentAgentIndex()]
        # get the opponent's color
        opponent_color = self.rule.agent_colors[self.rule.getNextAgentIndex()]
        Max_player_corner = 0
        Min_player_corner = 0
        for x in [0, GRID_SIZE - 1]:
            for y in [0, GRID_SIZE - 1]:
                if game_state[x][y] == current_player_color:
                    Max_player_corner += 1
                elif game_state[x][y] == opponent_color:
                    Min_player_corner += 1
        if (Max_player_corner + Min_player_corner) != 0:
            h_value = (Max_player_corner - Min_player_corner) * 100 / (Max_player_corner + Min_player_corner)
        else:
            h_value = 0
        return h_value




    def generate_static_weights(self, times):
            """
            The static weights are used to evaluate the board. Because some place is more important than others, we give them
            We can adjust the times of the weights to make the agent more clear that some places are important.

            Basic value is like the following:
            4  -3  2  2  2  2  -3  4
           -3  -4 -1 -1 -1 -1  -4 -3
            2  -1  1  0  0  1  -1  2
            2  -1  0  1  1  0  -1  2
            2  -1  0  1  1  0  -1  2
            2  -1  1  0  0  1  -1  2
           -3  -4 -1 -1 -1 -1  -4 -3
            4  -3  2  2  2  2  -3  4
            """
            # create a 8*8 matrix
            weights = [[0 for i in range(GRID_SIZE)] for j in range(GRID_SIZE)]

            # the places in the corner are more important than the places in the edge
            # so, we give the corner places a higher weight - 4
            for x in [0, GRID_SIZE - 1]:
                for y in [0, GRID_SIZE - 1]:
                    weights[x][y] = 4 * times

            # the places in the edge are less important(except cells beside corner) than the places in the corner
            # so, we give the edge places a lower weight - 2
            for x in range(2, GRID_SIZE - 2):
                weights[x][0] = 2 * times
                weights[x][GRID_SIZE - 1] = 2 * times
            for y in range(2, GRID_SIZE - 2):
                weights[0][y] = 2 * times
                weights[GRID_SIZE - 1][y] = 2 * times

            # the places in the middle are less important than the places in the edge
            # so, we give the middle places a lower weight - 1
            for x in [3,4]:
                for y in [3,4]:
                    weights[x][y] = 1 * times
            for x in [2,5]:
                for y in [2,5]:
                    weights[x][y] = 1 * times

            # places value = 0
            for x in [3, 4]:
                for y in [2, 5]:
                    weights[x][y] = 0
            for x in [2, 5]:
                for y in [3, 4]:
                    weights[x][y] = 0

            # places value = -1
            for x in range(2, GRID_SIZE - 2):
                weights[x][1] = -1 * times
                weights[x][GRID_SIZE - 2] = -1 * times
            for y in range(2, GRID_SIZE - 2):
                weights[1][y] = -1 * times
                weights[GRID_SIZE - 2][y] = -1 * times

            # places value = -3
            weights[1][0] = -3 * times
            weights[0][1] = -3 * times
            weights[GRID_SIZE - 2][0] = -3 * times
            weights[GRID_SIZE - 1][1] = -3 * times
            weights[1][GRID_SIZE - 1] = -3 * times
            weights[0][GRID_SIZE - 2] = -3 * times
            weights[GRID_SIZE - 2][GRID_SIZE - 1] = -3 * times
            weights[GRID_SIZE - 1][GRID_SIZE - 2] = -3 * times

            # places value = -4
            weights[1][1] = -4 * times
            weights[GRID_SIZE - 2][1] = -4 * times
            weights[1][GRID_SIZE - 2] = -4 * times
            weights[GRID_SIZE - 2][GRID_SIZE - 2] = -4 * times

            return weights






# #  alpha beta pruning
# def minimax(self, gameState, depth, alpha, beta, maximizingPlayer):
#     # if the game is over or the depth is 0, return the score
#     if depth == 0 or gameState.isWin() or gameState.isLose():
#         return gameState.getScore()
#
#     # if it is the maximizing player
#     if maximizingPlayer:
#         max_value = -float("inf")
#         for action in gameState.getLegalActions(0):
#             # get the value of the action
#             value = self.minimax(gameState.generateSuccessor(0, action), depth - 1, alpha, beta, False)
#             max_value = max(value, max_value)
#             alpha = max(alpha, value)
#             if alpha >= beta:
#                 break
#         return max_value
#
#     # if it is the minimizing player
#     else:
#         min_value = float("inf")
#         for action in gameState.getLegalActions(1):
#             value = self.minimax(gameState.generateSuccessor(1, action), depth - 1, alpha, beta, True)
#             min_value = min(min_value, value)
#             beta = min(beta, value)
#             if alpha >= beta:
#                 break
#         return min_value